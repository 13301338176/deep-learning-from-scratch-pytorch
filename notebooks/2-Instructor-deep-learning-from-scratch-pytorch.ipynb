{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having spent spent some time working on the implementation of supervised learning and getting familiar with the terminology of neural networks, let's write some code to implement a neural network from scratch. We're going to use a functional programming style to help build intuition. To make matters easier, we'll use a dictionary called `model` to store all data associated with the neural network (the weight matrices, the  bias vectors, etc.) and pass that into functions as a single argument. Production codes usually use an object-oriented style to  build networks and, of course, are optimized for efficiency (unlike what we'll develop here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to borrow notation from Michael Neilsen's [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com) to make life easier. In particular, we'll let $W^\\ell$ and $b^\\ell$ denote the weight matrices associated with the $\\ell$th layer of the network. The entry $W^{\\ell}_{jk}$ of $W^\\ell$ is the weight parameter associated with the link connecting the $k$th neuron in layer $\\ell-1$ to the $j$th neuron in layer $\\ell$:\n",
    "\n",
    "[![](img/tikz16.png)](http://neuralnetworksanddeeplearning.com/chap2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create an initialization function to set up model\n",
    "\n",
    "Rather than the fixed constants in `setup` from before, write a function `initialize_model` that accepts a list  `dimensions` of positive integer inputs that constructs a `dict` with specific key-value pairs:\n",
    "+ `model['nlayers']` : number of layers in neural network\n",
    "+ `model['weights']` : list of NumPy matrices with appropriate dimensions\n",
    "+ `model['biases']` : list of NumPy (column) vectors of appropriate dimensions\n",
    "\n",
    "lists `weights` and `biases` with weight matrices and bias vectors of appropriate dimensions as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(dimensions):\n",
    "    '''Accepts a list of positive integers; returns a dict 'model' with key/values as follows:\n",
    "      model['nlayers'] : number of layers in neural network\n",
    "      model['weights'] : list of NumPy matrices with appropriate dimensions\n",
    "      model['biases'] : list of NumPy (column) vectors of appropriate dimensions\n",
    "    These correspond to the weight matrices & bias vectors associated with each layer of a neural network.'''\n",
    "    weights, biases = [], []\n",
    "    L = len(dimensions) - 1 # number of layers (i.e., excludes input layer)\n",
    "    for l in range(L):\n",
    "        W = np.random.randn(dimensions[l+1], dimensions[l])\n",
    "        b = np.random.randn(dimensions[l+1], 1)\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    return dict(weights=weights, biases=biases, nlayers=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:\tShape of W1: (15, 784)\tShape of b1: (15, 1)\n",
      "Layer 2:\tShape of W2: (10, 15)\tShape of b2: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Example to verify the preceding:\n",
    "# For MNIST, consider dimensions = [28*28, 15, 10]\n",
    "dimensions = [28*28, 15, 10]\n",
    "model = initialize_model(dimensions)\n",
    "for k, (W, b) in enumerate(zip(model['weights'], model['biases'])):\n",
    "    print(f'Layer {k+1}:\\tShape of W{k+1}: {W.shape}\\tShape of b{k+1}: {b.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement activation function(s), loss functions, & their derivatives\n",
    "For today's purposes, we'll use only the *logistic* or *sigmoid* function as an activation function:\n",
    "$$ \\sigma(x) = \\frac{1}{1+\\exp(-x)} = \\frac{\\exp(x)}{1+\\exp(x)}.$$\n",
    "A bit of calculus shows that that\n",
    "$$ \\sigma'(x) = \\sigma(x)(1-\\sigma(x)) .$$\n",
    "Later, we'll see how PyTorch lets us use different ones. For the loss function, we'll use the typical \"$L_2$-norm of the error\" (alternatively called *mean-square error (MSE)* when averaged over a batch of values:\n",
    "$$ \\mathcal{E}(\\hat{y},y) = \\frac{1}{2} \\|\\hat{y}-y\\|^{2} = \\frac{1}{2} \\sum_{k=1}^{d} \\left[ \\hat{y}_{k}-y_{k} \\right]^{2}.$$\n",
    "Again, using multivariable calculus, we can see that\n",
    "$$\\nabla \\mathcal{E}(\\hat{y},y) = \\hat{y} - y.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    '''The logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    return 1.0 / (1.0 + np.exp(-x))  # naive logistic function implementation\n",
    "def g_prime(x):\n",
    "    '''The *derivative* of the logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    return g(x)*(1-g(x)) # Derivative of logistic function\n",
    "\n",
    "def loss(yhat, y):\n",
    "    '''The loss as measured by the L2-norm squared of the error'''\n",
    "    return 0.5 * np.square(yhat-y).sum()\n",
    "def loss_prime(yhat, y):\n",
    "    '''Implementation of the gradient of the loss function'''\n",
    "    return (yhat - y) # gradient w.r.t. yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement a function for forward propagation\n",
    "\n",
    "Write a function `forward` that uses the architecture described in a `dict` as created by `initialize_model` to evaluate the output of the neural network for a given input *column* vector `x`.\n",
    "+ Take $a^{0}=x$ from the input.\n",
    "+ For $\\ell=1,\\dotsc,L$, compute & store the intermediate computed vectors $z^{\\ell}=W^{\\ell}a^{\\ell-1}+b^{\\ell}$ (the *weighted inputs*) and $a^{\\ell}=\\sigma\\left(z^{\\ell}\\right)$ (the *activations*) in an updated dictionary `model`. That is, modify the input dictionary `model` so as to accumulate:\n",
    "  + `model['activations']`: a list with entries $a^{\\ell}$ for $\\ell=0,\\dotsc,L$\n",
    "  + `model['z_inputs']`: a list with entries $z^{\\ell}$ for $\\ell=1,\\dotsc,L$\n",
    "+ The function should return the computed output $a^{L}$ and the modified dictionary `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract process into function and run tests again.\n",
    "def forward(x, model):\n",
    "    '''Implementation of forward propagation'''\n",
    "    a = x\n",
    "    activations = [a]\n",
    "    zs = []\n",
    "    for W, b in zip(model['weights'], model['biases']):\n",
    "        z = W @ a + b\n",
    "        a = g(z)\n",
    "        zs.append(z)\n",
    "        activations.append(a)\n",
    "    model['activations'], model['w_inputs'] = activations, zs\n",
    "    return (a, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['weights', 'biases', 'nlayers', 'activations', 'w_inputs'])\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "# For MNIST, consider dimensions = [28*28, 15, 10]\n",
    "dimensions = [28*28, 15, 10]\n",
    "model = initialize_model(dimensions)\n",
    "yhat, model = forward(np.ones((784,2)), model)\n",
    "# Verify that new key-value pairs were created:\n",
    "print(model.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm for backpropagation:\n",
    "\n",
    "The description here is based on the *wonderfully concise* description from Michael Neilsen's [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com/chap2.html). Neilsen has artfully crafted a summary using the bare minimum mathematical prerequisites. The notation elegantly summarises the important ideas in a way to make implementation easy in array-based frameworks like Matlab or NumPy. This is the best description I (Dhavide) know of that does this.\n",
    "\n",
    "In the following, the symbol $\\odot$ is a fancy way of writing the [*Hadamard product*](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) of two conforming arrays; this is simply the usual element-wise product of arrays as computed by NumPy & is sometimes called the *Schur product*. This can be reformulated in usual matrix algebra for analysis.\n",
    "\n",
    "Given a neural network with $L$ layers (not including the \"input layer\") described by an appropriate architecture:\n",
    "\n",
    "1. Input $x$: Set the corresponding activation $a^{0} \\leftarrow x$ for the input layer.\n",
    "2. Feedforward: For each $\\ell=1,2,\\dotsc,L$, compute *weighted inputs* $z^{\\ell}$ & *activations* $a^{\\ell}$ using the formulas\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z^{\\ell} & \\leftarrow  W^{\\ell} a^{\\ell-1} + b^{\\ell}, \\\\\n",
    "a^{\\ell} & \\leftarrow  \\sigma\\left( z^{\\ell}\\right)\n",
    "\\end{aligned}.\n",
    "$$\n",
    "3. Starting from the end, compute the \"error\" in the output layer $\\delta^{L}$ according to the formula\n",
    "$$\n",
    "\\delta^{L} \\leftarrow \\nabla_{a^{L}} \\mathcal{E} \\odot \\sigma'\\left(z^{L}\\right)\n",
    "$$\n",
    "\n",
    "4. *Backpropagate* the \"error\" for $\\ell=Lâˆ’1\\dotsc,1$ using the formula\n",
    "$$\n",
    "\\delta^{\\ell} \\leftarrow \\left[ W^{\\ell+1}\\right]^{T}\\delta^{\\ell+1} \\odot \\sigma'\\left(z^{\\ell}\\right).\n",
    "$$\n",
    "5. The required gradients of the loss function $\\mathcal{E}$ with respect to the parameters $W^{\\ell}_{p,q}$ and $b^{\\ell}_{r}$ can be computed directly from the \"errors\" $\\left\\{ \\delta^{\\ell} \\right\\}$ and the weighted inputs $\\left\\{ z^{\\ell} \\right\\}$ according to the relations\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\frac{\\partial \\mathcal{E}}{\\partial W^{\\ell}_{p,q}} &= a^{\\ell-1}_{q} \\delta^{\\ell}_{p} &&(\\ell=1,\\dotsc,L)\\\\\n",
    "   \\frac{\\partial \\mathcal{E}}{\\partial b^{\\ell}_{r}} &= \\delta^{\\ell}_{r} &&\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement a function for backward propagation\n",
    "Implement a function `backward` that implements the back-propagation algorithm to compute the gradients of the loss function $\\mathcal{E}$ with respect to the weight matrices $W^{\\ell}$ and the bias vectors $b^{\\ell}$.\n",
    "+ The function should accept a column vector `y` of output labels and an appropriate dictionary `model` as input.\n",
    "+ The dict `model` is assumed to have been generated *after* a call to `forward`; that is, `model` should have keys `'w_inputs'` and `'activations'` as computed by a call to `forward`.\n",
    "+ The result will be a modified dictionary `model` with two additional key-value pairs:\n",
    "  + `model['grad_weights']`: a list with entries $\\nabla_{W^{\\ell}} \\mathcal{E}$ for $\\ell=1,\\dotsc,L$\n",
    "  + `model['grad_biases']`: a list with entries $\\nabla_{b^{\\ell}} \\mathcal{E}$ for $\\ell=1,\\dotsc,L$\n",
    "+ Notice the dimensions of the matrices $\\nabla_{W^{\\ell}}$ and the vectors $\\nabla_{b^{\\ell}}$ will be identical to those of ${W^{\\ell}}$ and ${b^{\\ell}}$ respectively.\n",
    "+ The function's return value is the modified dictionary `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(y, model):\n",
    "    '''Implementation of backward propagation of data through the network'''\n",
    "    Nbatch = y.shape[1] # Needed to extend for batches of vectors\n",
    "    yhat = model['activations'][-1]\n",
    "    z, a = model['w_inputs'][-1], model['activations'][-2]\n",
    "    delta = loss_prime(yhat, y) * g_prime(z)\n",
    "    grad_b, grad_W = delta @ np.ones((Nbatch, 1)), np.dot(delta, a.T)\n",
    "    grad_weights, grad_biases = [grad_W], [grad_b]\n",
    "    loop_iterates = zip(model['weights'][-1:0:-1],\n",
    "                        model['w_inputs'][-2::-1],\n",
    "                        model['activations'][-3::-1])\n",
    "    for W, z, a in loop_iterates:\n",
    "        delta = np.dot(W.T, delta) * g_prime(z)\n",
    "        grad_b, grad_W = delta @ np.ones((Nbatch, 1)), np.dot(delta, a.T)\n",
    "        grad_weights.append(grad_W)\n",
    "        grad_biases.append(grad_b)\n",
    "    # We built up lists of gradients backwards, so we reverse the lists\n",
    "    model['grad_weights'], model['grad_biases'] = grad_weights[::-1], grad_biases[::-1]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement a function to update the model parameters using computed gradients.\n",
    "\n",
    "Given some positive learning rate $\\eta>0$, we want to change all the weights and biases using their gradients.\n",
    "Write a function `update` to compute a single step of gradient descent assuming that the model gradients have been computed for a given input vector.\n",
    "+ The functions signature should be `update(eta, model)` where `eta` is a positive scalar value and `model` is a dictionary as output from `backward`.\n",
    "+ The result will be an updated model with the values updated for `model['']` and `model['']`.\n",
    "+ Written using array notations, these updates can be expressed as\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   W^{\\ell} &\\leftarrow W^{\\ell} - \\eta \\nabla_{W^{\\ell}} \\mathcal{E} &&(\\ell=1,\\dotsc,L)\\\\\n",
    "   b^{\\ell} &\\leftarrow b^{\\ell} - \\eta \\nabla_{W^{\\ell}} \\mathcal{E} &&\n",
    "   \\end{aligned}.\n",
    "   $$\n",
    "+ Written out component-wise, the preceding array expressions would be written as\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "      W^{\\ell}_{p,q} &\\leftarrow W^{\\ell}_{p,q} - \\eta \\frac{\\partial \\mathcal{E}}{\\partial W^{\\ell}_{p,q}}\n",
    "      &&(\\ell=1,\\dotsc,L)\\\\\n",
    "      b^{\\ell}_{r} &\\leftarrow b^{\\ell}_{r} - \\eta \\frac{\\partial \\mathcal{E}}{\\partial b^{\\ell}_{r}} &&\n",
    "   \\end{aligned}\n",
    "   $$.\n",
    "+ For safety, have the update step replace the keys `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(eta, model):\n",
    "    '''Use learning rate and gradients to update model parameters'''\n",
    "    # Get rid of extraneous keys/values\n",
    "    for key in ['activations', 'w_inputs', 'grad_weights', 'grad_biases']:\n",
    "        del model[key]\n",
    "    new_weights, new_biases = [], []\n",
    "    for W, b, dW, db in zip(model['weights'], model['biases'], model['grad_weights'], model['grad_biases']):\n",
    "        new_weights.append(W - (eta * dW))\n",
    "        new_biases.append(b- (eta * db))\n",
    "    model['weights'] = new_weights\n",
    "    model['biases'] = new_biases\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = update(0.5, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement steepest descent in a loop for random training data\n",
    "\n",
    "Let's now attempt to use our NumPy-based model to implement the steepest descent algorithm. We'll explain these numbers shortly in the context of the MNIST digit classification problem.\n",
    "\n",
    "+ Generate random arrays `X` and `y` of dimensions $28^2 \\times N_{\\mathrm{batch}}$ and $10\\times N_{\\mathrm{batch}}$ respectively.\n",
    "+ Initialize the network architecture using `initialize_model` as above to require an input layer of $28^2$ units, a hidden layer of 15 units, and an output layer of 10 units.\n",
    "+ Choose a learning rate of, say, $\\eta=0.5$ and a number of epochs `n_epoch` of, say, $30$.\n",
    "+ Construct a for loop with `n_epochs` iterations in which:\n",
    "    + The output `yhat` is computed from the input`X` using `forward`.\n",
    "    + The function `backward` is called to compute the gradients of the loss function with respect to the weights and biases.\n",
    "    + Update the network parameters using the function `update`.\n",
    "    + Compute and print out the epoch (iteration counter) and the value of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 20.989553151655162\n",
      "Epoch: 1\tLoss: 6.853732384383584\n",
      "Epoch: 2\tLoss: 0.41476269160901136\n",
      "Epoch: 3\tLoss: 0.06944517160310343\n",
      "Epoch: 4\tLoss: 0.041046872910784134\n",
      "Epoch: 5\tLoss: 0.033729596459397604\n",
      "Epoch: 6\tLoss: 0.029225443938717365\n",
      "Epoch: 7\tLoss: 0.026014307464333437\n",
      "Epoch: 8\tLoss: 0.02356193949144214\n",
      "Epoch: 9\tLoss: 0.021607371469112357\n",
      "Epoch: 10\tLoss: 0.020002266183459128\n",
      "Epoch: 11\tLoss: 0.018654269624344334\n",
      "Epoch: 12\tLoss: 0.01750217606665738\n",
      "Epoch: 13\tLoss: 0.016503497937321875\n",
      "Epoch: 14\tLoss: 0.015627654079678817\n",
      "Epoch: 15\tLoss: 0.014851978941989362\n",
      "Epoch: 16\tLoss: 0.014159260191572787\n",
      "Epoch: 17\tLoss: 0.013536154855007134\n",
      "Epoch: 18\tLoss: 0.012972134954011498\n",
      "Epoch: 19\tLoss: 0.01245876498796973\n",
      "Epoch: 20\tLoss: 0.011989194339661514\n",
      "Epoch: 21\tLoss: 0.011557792848766206\n",
      "Epoch: 22\tLoss: 0.011159884114320201\n",
      "Epoch: 23\tLoss: 0.010791546959750077\n",
      "Epoch: 24\tLoss: 0.010449465357905363\n",
      "Epoch: 25\tLoss: 0.010130813405847615\n",
      "Epoch: 26\tLoss: 0.009833166047222924\n",
      "Epoch: 27\tLoss: 0.009554428978189835\n",
      "Epoch: 28\tLoss: 0.00929278303233221\n",
      "Epoch: 29\tLoss: 0.009046639624345341\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(28*28, 13)\n",
    "y = np.zeros((10,13)); y[5]=1\n",
    "eta = 0.5\n",
    "model = initialize_model([28*28, 15, 10])\n",
    "\n",
    "n_epochs = 30\n",
    "for epoch in range(n_epochs):\n",
    "    yhat, model = forward(X, model)\n",
    "    model = backward(y, model)\n",
    "    model = update(eta, model)\n",
    "    err = loss(yhat, y)\n",
    "    print(f'Epoch: {epoch}\\tLoss: {err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modify the steepest descent loop to make a plot\n",
    "\n",
    "Let's alter the preceding loop to accumulate selected epoch & loss values in lists for plotting.\n",
    "\n",
    "+ Set `n_epochs` to be larger, say, $30,000$.\n",
    "+ Change the preceding `for` loop so that:\n",
    "    + The `epoch` counter and the loss value are accumulated into lists every, say, 500 iterations.\n",
    "    + Eliminate the `print` statement(s) to save on output.\n",
    "+ After the `for` loop terminates, make a `semilogy` plot to verify that the loss function is actually decreasing with sucessive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(28*28, 13)\n",
    "y = np.zeros((10,13)); y[5]=1\n",
    "eta = 0.5\n",
    "model = initialize_model([28*28, 15, 10])\n",
    "\n",
    "epochs, losses = [], []\n",
    "n_epochs = 30000\n",
    "for epoch in range(n_epochs):\n",
    "    yhat, model = forward(X, model)\n",
    "    model = backward(y, model)\n",
    "    model = update(eta, model)\n",
    "    if (divmod(epoch, 500)[1]==0):\n",
    "        err = loss(yhat, y)\n",
    "        epochs.append(epoch)\n",
    "        losses.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(); ax = fig.add_subplot(111)\n",
    "ax.set_xlim([0,n_epochs]); ax.set_ylim([min(losses), max(losses)]);\n",
    "ax.set_xticks(epochs[::500]); ax.set_xlabel(\"Epochs\"); ax.grid(True);\n",
    "ax.set_ylabel(r'$\\mathcal{E}$'); \n",
    "h1 = ax.semilogy(epochs, losses, 'r-', label=r'$\\mathcal{E}$')\n",
    "plt.title('Loss vs. epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST data set](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
